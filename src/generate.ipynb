{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:33:54.287786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747132434.315831   30515 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747132434.324026   30515 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-13 12:33:54.350479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import random\n",
    "# Use multiple models at the same time with the same eps for untargeted and targeted\n",
    "# Try out with different labels for the targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:12:43.084237: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
    "                                                     weights='imagenet')\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "# ImageNet labels\n",
    "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to preprocess the image so that it can be inputted in MobileNetV2\n",
    "def preprocess(image):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = tf.image.resize(image, (224, 224))\n",
    "  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "  image = image[None, ...]\n",
    "  return image\n",
    "\n",
    "# Helper function to extract labels from probability vector\n",
    "def get_imagenet_label(probs):\n",
    "  return decode_predictions(probs, top=1)[0][0]\n",
    "\n",
    "def display_images(image, description):\n",
    "  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))\n",
    "  plt.figure()\n",
    "  plt.imshow(image[0]*0.5+0.5)\n",
    "  plt.title('{} \\n {} : {:.2f}% Confidence'.format(description,\n",
    "                                                   label, confidence*100))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clipping the input vector\n",
    "def clip_delta(t, eps):\n",
    "    return tf.clip_by_value(t, -eps, eps)\n",
    "\n",
    "\n",
    "# Generates targeted adversarial images\n",
    "def generate_adv_pertubation(model, optimizer, loss, img, delta, label, target_label, eps, max_iterations=500):\n",
    "    # delta = tf.Variable(tf.zeros_like(img), trainable=True)\n",
    "    \n",
    "    for step in range(max_iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(delta)\n",
    "            adv = preprocess(img + delta)\n",
    "\n",
    "            preds = model(adv,training=False)\n",
    "            # print(preds)\n",
    "            originalLoss = -loss(tf.convert_to_tensor([label]), preds)\n",
    "            targetLoss = loss(tf.convert_to_tensor([target_label]), preds)\n",
    "            totalLoss = originalLoss + targetLoss\n",
    "\n",
    "            # if step % 50 == 0:\n",
    "            #     print(\"step: {}, loss: {}...\".format(step, totalLoss.numpy()), \"Range:\", tf.reduce_min(adv).numpy(), \"to\", tf.reduce_max(adv).numpy())\n",
    "\n",
    "            gradients = tape.gradient(totalLoss, delta)\n",
    "            optimizer.apply_gradients([(gradients, delta)])\n",
    "            delta.assign_add(clip_delta(delta, eps))\n",
    "    \n",
    "    return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(tensor, target_folder, filename):\n",
    "\n",
    "    # Remove batch dimension if present\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor[0]\n",
    "\n",
    "    # Convert from [-1, 1] to [0, 255]\n",
    "    tensor = (tensor + 1.0) / 2.0  # [-1,1] â†’ [0,1]\n",
    "    tensor = tf.clip_by_value(tensor, 0.0, 1.0)\n",
    "    tensor = tf.image.convert_image_dtype(tensor, dtype=tf.uint8)\n",
    "\n",
    "    # Encode as JPEG\n",
    "    encoded = tf.io.encode_jpeg(tensor)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "    # Save to file\n",
    "    path = os.path.join(target_folder, filename)\n",
    "    tf.io.write_file(path, encoded)\n",
    "    print(f\"Image saved to: {path}\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    raw = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(raw, channels=3)\n",
    "    return tf.cast(img, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_images(images, source_folder, target_folder, label, target_label, eps, iterations):\n",
    "    i, mx = 0, len(images)\n",
    "    for img_name in images:\n",
    "        print(f\"{i}/{mx}: {img_name}\")\n",
    "        image_path = tf.keras.utils.get_file(img_name, 'file://' + source_folder + img_name)\n",
    "        image_raw = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_image(image_raw)\n",
    "        \n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        adam = tf.keras.optimizers.Adam(1e-2)\n",
    "        \n",
    "        baseImage = tf.constant(image / 1, dtype=tf.float32)\n",
    "        delta = tf.Variable(tf.zeros_like(baseImage), trainable=True)\n",
    "\n",
    "        target_pertubation = generate_adv_pertubation(pretrained_model, adam, loss, baseImage, delta, label, target_label, eps, iterations)\n",
    "\n",
    "        adv = (baseImage + target_pertubation)\n",
    "        adv = preprocess(adv)\n",
    "\n",
    "        _, cl, conf = get_imagenet_label(pretrained_model.predict(adv))\n",
    "        # display_images(adv, descriptions[i])\n",
    "        print(f\"Classified as {cl} with {conf}% confidence.\")\n",
    "        \n",
    "        # if cl == \"pineapple\":\n",
    "        save_image(adv, target_folder, img_name)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 17:23:05.589357: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Shapes of all inputs must match: values[0].shape = [333,500,3] != values[1].shape = [460,640,3]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_100_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [333,500,3] != values[1].shape = [460,640,3] [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m target_label \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m953\u001b[39m)\n\u001b[1;32m     10\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mgenerate_adversarial_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m, in \u001b[0;36mgenerate_adversarial_images\u001b[0;34m(images, source_folder, target_folder, label, target_label, eps, iterations)\u001b[0m\n\u001b[1;32m      2\u001b[0m i, mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[1;32m      4\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile://\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m source_folder \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m----> 5\u001b[0m images_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mload_and_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images_tensor:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_100_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [333,500,3] != values[1].shape = [460,640,3] [Op:Pack] name: stack"
     ]
    }
   ],
   "source": [
    "full_path = \"./banana_images/data/train/banana/\"\n",
    "target_path = \"./adversarial_images/\"\n",
    "image_files = [f for f in os.listdir(full_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "sub_images = image_files[:100]\n",
    "\n",
    "eps = 0.01\n",
    "label = tf.Variable(954)\n",
    "target_label = tf.Variable(953)\n",
    "iterations = 100\n",
    "\n",
    "generate_adversarial_images(sub_images, full_path, target_path, label, target_label, eps, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untargeted attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversarial_pattern(input_image, input_label, loss_object, model):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(input_image)\n",
    "    prediction = model(input_image)\n",
    "    loss = loss_object(input_label, prediction)\n",
    "\n",
    "  # Get the gradients of the loss w.r.t to the input image.\n",
    "  gradient = tape.gradient(loss, input_image)\n",
    "  # Get the sign of the gradients to create the perturbation\n",
    "  signed_grad = tf.sign(gradient)\n",
    "  return signed_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_untargeted_adversarial_images(images, source_folder, target_folder, label, eps):\n",
    "    i, mx = 0, len(images)\n",
    "    label = tf.one_hot(label, 1000)\n",
    "    label = tf.reshape(label, (1, 1000))\n",
    "    for img_name in images:\n",
    "        print(f\"{i}/{mx}: {img_name}\")\n",
    "\n",
    "        image_path = tf.keras.utils.get_file(img_name, 'file://' + source_folder + img_name)\n",
    "        image_raw = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_image(image_raw)\n",
    "\n",
    "        image_pr = preprocess(image)\n",
    "        \n",
    "        loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        pertubation = create_adversarial_pattern(image_pr, label, loss_object, pretrained_model)\n",
    "\n",
    "        adv = image_pr + eps * pertubation\n",
    "        adv = tf.clip_by_value(adv, -1, 1)\n",
    "\n",
    "        _, cl, conf = get_imagenet_label(pretrained_model.predict(adv))\n",
    "        # display_images(adv, descriptions[i])\n",
    "        print(f\"Classified as {cl} with {conf}% confidence.\")\n",
    "\n",
    "        save_image(adv, target_folder, img_name)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/applications/mobilenet_v3.py:452: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  return MobileNetV3(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3, EfficientNetB0, MobileNetV3Small, MobileNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as res_pre, decode_predictions as res_decode\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_pre, decode_predictions as vgg_decode\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inc_pre, decode_predictions as inc_decode\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input as mn_pre, decode_predictions as mn_decode\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mn2_pre, decode_predictions as mn2_decode\n",
    "\n",
    "model_names = (\"ResNet50\", \"VGG\", \"MobileNetV3Small\")\n",
    "models = (MobileNetV2(weights='imagenet'), ResNet50(weights='imagenet'), VGG16(weights='imagenet'), MobileNetV3Small(weights='imagenet'))\n",
    "pre_functions = (res_pre, vgg_pre, inc_pre, mn_pre)\n",
    "decode_functions = (res_decode, vgg_decode, inc_decode, mn_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_untargeted_adversarial_images_multiple_models(images, source_folder, target_folder, label, eps, models, decoders, preprocessors):\n",
    "    i, mx = 0, len(images)\n",
    "    label = tf.one_hot(label, 1000)\n",
    "    label = tf.reshape(label, (1, 1000))\n",
    "    for img_name in images:\n",
    "        print(f\"{i}/{mx}: {img_name}\")\n",
    "\n",
    "        image_path = tf.keras.utils.get_file(img_name, 'file://' + source_folder + img_name)\n",
    "        image_raw = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_image(image_raw)\n",
    "        image_pr = preprocess(image)\n",
    "        for j in range(len(models)):\n",
    "            \n",
    "            loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "            pertubation = create_adversarial_pattern(image_pr, label, loss_object, models[j])\n",
    "            # plt.imshow(pertubation[0] * 0.5 + 0.5)\n",
    "            display_images(pertubation, \"\")\n",
    "\n",
    "            adv = image_pr + eps * pertubation\n",
    "            adv = tf.clip_by_value(adv, -1, 1)\n",
    "            image_pr = adv\n",
    "        print(f\"Dimensions: raw = {image_raw.shape}, img = {image.shape}, preprocessed = {image_pr.shape}, adv = {adv.shape}\")\n",
    "        _, cl, conf = get_imagenet_label(pretrained_model.predict(adv))\n",
    "        # display_images(adv,\"\")\n",
    "        print(f\"Classified as {cl} with {conf}% confidence.\")\n",
    "\n",
    "        save_image(adv, target_folder, img_name)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:35:23.621731: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_untargeted_adversarial_images_multiple_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m      8\u001b[0m label \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m954\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mgenerate_untargeted_adversarial_images_multiple_models\u001b[49m(sub_images, full_path, target_path, label, eps, models, decode_functions, pre_functions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_untargeted_adversarial_images_multiple_models' is not defined"
     ]
    }
   ],
   "source": [
    "full_path = \"./banana_images/data/train/banana/\"\n",
    "target_path = \"./adversarial_images_untargeted/\"\n",
    "image_files = [f for f in os.listdir(full_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:2]\n",
    "\n",
    "sub_images = image_files\n",
    "\n",
    "eps = 0.01\n",
    "label = tf.Variable(954)\n",
    "\n",
    "generate_untargeted_adversarial_images_multiple_models(sub_images, full_path, target_path, label, eps, models, decode_functions, pre_functions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
